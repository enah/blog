<?xml version="1.0" encoding="UTF-8" ?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en" lang="en">
    <head>
        <meta http-equiv="Content-Type" content="text/html; charset=UTF-8" />
        <title>Ena Hariyoshi's Blog - A Step-By-Step Guide to the Chernoff Bound</title>
        <link rel="stylesheet" type="text/css" href="../css/default.css" />
	<script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
    </head>
    <body>
        <div id="header">
            <div id="logo">
                <a href="../">Ena Hariyoshi's Blog</a>
            </div>
            <div id="navigation">
                <a href="../">Home</a>
                <a href="../about.html">About</a>
                <a href="../contact.html">Contact</a>
                <a href="../archive.html">Archive</a>
            </div>
        </div>

        <div id="content">
            <h1>A Step-By-Step Guide to the Chernoff Bound</h1>

            <div class="info">
    Posted on January  6, 2015
    
        by Ena Hariyoshi
    
</div>

<p>I TA-ed for the Discrete Mathematics and Probability course (CS70) in the EECS department at UC Berkeley in Fall 2014. Every year students have difficulty with Chernoff bounds; I thought I’d leave this here that future CS70 students - and others - can use this as a reference.</p>
<p>Some overview about the Chernoff Bound: The Chernoff Bound is derived by applying the Markov Bound to <span class="math"><em>M</em>(<em>s</em>) = E[<em>e</em><sup><em>s</em><em>X</em></sup>]</span>, which is the time-reversed <a href="http://en.wikipedia.org/wiki/Two-sided_Laplace_transform">bilateral Laplace transform</a> of the random variable <span class="math"><em>X</em></span> and is commonly called the <a href="http://en.wikipedia.org/wiki/Moment-generating_function">moment-generating function</a>.</p>
<p>A brief note about moments: the mean of a random variable is analogous to the first moment; the variance of a random variable is analogous to the second moment; third moment is skew; and so on. <span class="math"><em>M</em>(<em>s</em>) = E[<em>e</em><sup><em>s</em><em>X</em></sup>]</span> is a representation of all the moments, by Taylor expansion. Many probability distributions can be expressed as this representation of all the moments, but some cannot. If you’re curious you can read the Wikipedia page.<br />It is a transform in the same sense that the Fourier transform, or the Fourier series, takes a function and represents it as a series of coefficients of complex exponentials (sines and cosines).</p>
<p>This is a list you can work through step by step to derive the Chernoff Bound.</p>
<ol style="list-style-type: decimal">
<li>First, note that <span class="math">Pr(<em>X</em> ≥ <em>α</em>) = Pr(<em>e</em><sup><em>s</em><em>X</em></sup> ≥ <em>e</em><sup><em>s</em><em>α</em></sup>) ∀<em>s</em> ≥ 0</span><br />(Because <span class="math"><em>y</em> = <em>e</em><sup><em>s</em><em>x</em></sup></span> is strictly increasing).</li>
<li><br /><span class="math">$$\Pr(e^{sX} \geq e^{s\alpha}) \leq \frac{\mathbb{E}[e^{sX}]}{e^{s\alpha}}$$</span><br /> (Is <span class="math"><em>e</em><sup><em>s</em><em>X</em></sup></span> always positive?)</li>
<li>Minimize <br /><span class="math">$$\frac{\mathbb{E}[e^{sX}]}{e^{s\alpha}}$$</span><br /> over all <span class="math"><em>s</em> ≥ 0</span> to find the tightest bound.</li>
<li>This is the same as minimizing <br /><span class="math">$$\ln \frac{\mathbb{E}[e^{sX}]}{e^{s\alpha}} = \ln \mathbb{E}[e^{sX}] - s\alpha$$</span><br /> (Why is minimizing this the same? <span class="math"><em>y</em> = ln <em>x</em></span> is strictly increasing. Why do we do this? Which one is easier to minimize?)</li>
<li>When <span class="math"><em>X</em> = ∑<em>X</em><sub><em>i</em></sub></span> and <span class="math"><em>X</em><sub><em>i</em></sub></span> are independent, <span class="math">E[<em>e</em><sup><em>s</em><em>X</em></sup>] = ∏E[<em>e</em><sup><em>s</em><em>X</em><sub><em>i</em></sub></sup>]</span></li>
<li>When <span class="math"><em>X</em><sub><em>i</em></sub></span> are identical, <span class="math"> = E[<em>e</em><sup><em>s</em><em>X</em><sub><em>i</em></sub></sup>]<sup><em>n</em></sup></span></li>
<li>Then, <span class="math">lnE[<em>e</em><sup><em>s</em><em>X</em></sup>] − <em>s</em><em>α</em> = <em>n</em>lnE[<em>e</em><sup><em>s</em><em>X</em><sub><em>i</em></sub></sup>] − <em>s</em><em>α</em></span></li>
<li>Let <span class="math"><em>s</em>ʹ</span> be the <span class="math"><em>s</em></span> that minimizes <span class="math"><em>n</em>lnE[<em>e</em><sup><em>s</em><em>X</em><sub><em>i</em></sub></sup>] − <em>s</em><em>α</em></span>. Then, <br /><span class="math">$$n \frac{\partial \ln \mathbb{E}[e^{s'X_i}]}{\partial s'} = \alpha$$</span><br /></li>
<li>At this point, we are stuck unless we know the distribution of <span class="math"><em>X</em><sub><em>i</em></sub></span>. Let <span class="math"><em>X</em><sub><em>i</em></sub> ∼ <em>B</em><em>e</em><em>r</em><em>n</em><em>o</em><em>u</em><em>l</em><em>l</em><em>i</em>(<em>p</em>)</span>.<br />Then, <span class="math">E[<em>e</em><sup><em>s</em><em>X</em><sub><em>i</em></sub></sup>] = <em>p</em> ⋅ <em>e</em><sup><em>s</em> ⋅ 1</sup> + (1 − <em>p</em>) ⋅ <em>e</em><sup><em>s</em> ⋅ 0</sup> = <em>p</em><em>e</em><sup><em>s</em></sup> + 1 − <em>p</em></span></li>
<li>Solve for <br /><span class="math">$$n \frac{\partial \ln (pe^{s'} + 1 - p)}{\partial s'} = \alpha \\ \frac{npe^{s'}}{pe^{s'} + 1 - p} = \alpha \\ e^{s'} = \frac{\alpha}{n - \alpha}\frac{1-p}{p}$$</span><br /></li>
<li><br /><span class="math">$$e^{s'} = \frac{\alpha}{n - \alpha}\frac{1-p}{p}$$</span><br /> minimizes <br /><span class="math">$$\frac{\mathbb{E}[e^{sX}]}{e^{s\alpha}} = \frac{\mathbb{E}[e^{sX_i}]^n}{e^{s\alpha}} = \frac{(pe^s + 1 - p)^n}{e^{s\alpha}}$$</span><br /></li>
<li>Plug in, and massage into nicer forms. Let <span class="math"><em>a</em><em>n</em> = <em>α</em></span>. <br /><span class="math">$$\Pr(X \geq \alpha) \leq \frac{\left(\frac{\alpha(1-p)}{n-\alpha} + 1-p\right)^n}{\left(\frac{\alpha}{n-\alpha}\frac{1-p}{p}\right)^\alpha} = \frac{\left(\frac{a(1-p)}{1-a} + 1-p\right)^n}{\left(\frac{a}{1-a}\frac{1-p}{p}\right)^{\alpha}} \\ = \frac{\left(\frac{1-p}{n-a}\right)^n}{\left(\frac{1-p}{1-a}\frac{a}{p}\right)^{\alpha}} = \left(\frac{1-p}{1-a}\right)^{n-\alpha}\left(\frac{p}{a}\right)^{\alpha}$$</span><br /> As <span class="math"><em>n</em></span> gets large, for what values of <span class="math"><em>p</em></span> and <span class="math"><em>a</em></span> does <span class="math">Pr(<em>X</em> ≥ <em>α</em>)</span> get small?</li>
<li>Extra note: the bound is also said to be <span class="math"><em>e</em><sup> − <em>n</em>Φ<sub><em>X</em><sub><em>i</em></sub></sub>(<em>a</em>)</sup></span>, where <br /><span class="math">$$\Phi_{X_i}(a) = a \ln \frac{a}{p} + (1 - a) \ln \frac{1-a}{1-p}$$</span><br /> <span class="math">Φ<sub><em>X</em><sub><em>i</em></sub></sub>(<em>a</em>)</span> is known as the <a href="http://en.wikipedia.org/wiki/Kullback%E2%80%93Leibler_divergence">Kullback-Leibler divergence</a>.</li>
</ol>

        </div>
        <div id="footer">
            Site proudly generated by
            <a href="http://jaspervdj.be/hakyll">Hakyll</a>
        </div>
    </body>
</html>
